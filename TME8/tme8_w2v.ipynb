{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suite et fin du TME6: Classification de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "import codecs\n",
    "import nltk.corpus.reader as pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path1='data/movies1000/pos'\n",
    "path2='data/movies1000/neg'\n",
    "rdr1 = pt.CategorizedPlaintextCorpusReader(path1, r'.*\\.txt', cat_pattern=r'(.*)\\.txt')\n",
    "rdr2 = pt.CategorizedPlaintextCorpusReader(path2, r'.*\\.txt', cat_pattern=r'(.*)\\.txt')\n",
    "\n",
    "def make_training_data(rdr):\n",
    "    for c in rdr.categories():\n",
    "        for f in rdr.fileids(c):\n",
    "            yield rdr.raw(fileids=[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs1=list(make_training_data(rdr1))\n",
    "y1=[1 for i in range(len(docs1))]\n",
    "docs2=list(make_training_data(rdr2))\n",
    "y2=[-1 for i in range(len(docs2))]\n",
    "X_str=docs1+docs2\n",
    "y=y1+y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "\n",
    "stopw=readAFile('stopwords.txt')\n",
    "stopw=stopw.split()\n",
    "\n",
    "def process(txt,stopw=None):\n",
    "    #txt = txt[txt.find(\"\\n\\n\"):] # elimination de l'entete (on ne conserve que les caractères après la première occurence du motif\n",
    "    txt = unicodedata.normalize(\"NFKD\",txt).encode(\"ascii\",\"ignore\") # elimination des caractères spéciaux, accents...\n",
    "    punc = string.punctuation    # recupération de la ponctuation\n",
    "    punc += u'\\n\\r\\t\\\\'          # ajouts de caractères à enlever\n",
    "    table =string.maketrans(punc, ' '*len(punc))  # table de conversion punc -> espace\n",
    "    txt = string.translate(txt,table).lower() # elimination des accents + minuscules\n",
    "    \n",
    "    #stemming\n",
    "    txt_list=txt.split()\n",
    "    snowball = stem.snowball.EnglishStemmer()\n",
    "    txt_list=[snowball.stem(w) for w in txt_list]\n",
    "    if(stopw):\n",
    "        txt_list=[w for w in txt_list if(w not in stopw)]\n",
    "    txt=' '.join(txt_list)\n",
    "    \n",
    "    return txt\n",
    "\n",
    "X_str=[process(x_str) for x_str in X_str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readAFile(nf):\n",
    "    f = open(nf, 'rb')\n",
    "    l = []\n",
    "    txt = f.readlines()\n",
    "    for i in txt:\n",
    "        l.append(i.decode(\"utf-8\"))\n",
    "    f.close()\n",
    "    return ' '.join(l)\n",
    "\n",
    "path_test='data/movies1000/test/'\n",
    "docs_test = readAFile(path_test+\"testSentiment.txt\")\n",
    "docs_test = docs_test.split('\\n')[0:-1]\n",
    "docs_test=[process(doc_test) for doc_test in docs_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "countVe = TfidfVectorizer(max_df=0.55, min_df=1, #max_features=1000,\n",
    "                    )\n",
    "count = countVe.fit_transform(X_str)\n",
    "X=count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "C = 2.6  # SVM regularization parameter\n",
    "nb = svm.LinearSVC(C=C,max_iter=9000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_test = countVe.transform(docs_test)\n",
    "pred_labels=nb.predict(count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print len(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('sentim.txt', 'w')\n",
    "for i in pred_labels:\n",
    "    if i == -1:\n",
    "        f.write('C\\n')\n",
    "    else:\n",
    "        f.write('M\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([u'enjoy', u'fun', u'great', u'hilari', u'job', u'matrix',\n",
      "       u'perfect', u'perform', u'surpris', u'well'], \n",
      "      dtype='<U25')]\n"
     ]
    }
   ],
   "source": [
    "w=nb.coef_[0].argsort()\n",
    "\n",
    "neg=np.array(w[:10])\n",
    "pos=np.array(w[-10:])\n",
    "test=np.zeros((len(w)))\n",
    "test[pos]=1\n",
    "ic=countVe.inverse_transform(test)\n",
    "print ic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME8: word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "\n",
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "\n",
    "path1='data/movies1000/all'\n",
    "sentences = MySentences(path1) # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('season.', 0.646028459072113),\n",
       " ('night!', 0.6455487012863159),\n",
       " ('theater.', 0.6366403102874756),\n",
       " ('week.', 0.6299988031387329),\n",
       " ('weekend.', 0.6183199286460876),\n",
       " ('miniseries.', 0.6166147589683533),\n",
       " ('evening.', 0.614005982875824),\n",
       " ('day.', 0.6085823178291321),\n",
       " (\"'70s.\", 0.6062525510787964),\n",
       " ('list.', 0.5956918001174927)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['weekend',], negative=['sunday'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
