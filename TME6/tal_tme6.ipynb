{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tme6 TAL: Classification supervisée de documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "import unicodedata\n",
    "import re\n",
    "import codecs\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import svm\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.linear_model\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readAFile(nf):\n",
    "    '''reads a file and extracts a list of strings \n",
    "        representing the ligns of the document.'''\n",
    "    f = open(nf, 'rb')\n",
    "    l = []\n",
    "    txt = f.readlines()\n",
    "    for i in txt:\n",
    "        l.append(i.decode(\"utf-8\"))\n",
    "    f.close()\n",
    "    return l\n",
    "\n",
    "\n",
    "def process(txt):\n",
    "    '''preprocessing of a string:\n",
    "            * decoding and ascii encoding\n",
    "            * punctuation elimination\n",
    "            * letters to lowercase\n",
    "    '''\n",
    "    #txt = txt[txt.find(\"\\n\\n\"):] # elimination de l'entete (on ne conserve que les caractères après la première occurence du motif\n",
    "    txt = unicodedata.normalize(\"NFKD\",txt).encode(\"ascii\",\"ignore\") # elimination des caractères spéciaux, accents...\n",
    "    punc = string.punctuation    # recupération de la ponctuation\n",
    "    punc += u'\\n\\r\\t\\\\'          # ajouts de caractères à enlever\n",
    "    table =string.maketrans(punc, ' '*len(punc))  # table de conversion punc -> espace\n",
    "    txt = string.translate(txt,table).lower() # elimination des accents + minuscules\n",
    "    #return re.sub(\" +\",\" \", txt) # expression régulière pour transformer les espaces multiples en simples espaces\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = readAFile('train.utf8')\n",
    "\n",
    "#extracting text and labels\n",
    "x_raw = []\n",
    "y = []\n",
    "for txt in t:\n",
    "    lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "    txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "    x_raw.append(txt)\n",
    "    if('C' in lab):\n",
    "        bin_lab = 1\n",
    "    elif('M' in lab):\n",
    "        bin_lab = 0\n",
    "    else:\n",
    "        bin_lab = 'err'\n",
    "    y.append(bin_lab)\n",
    "\n",
    "#preprocessing\n",
    "x = [process(xi) for xi in x_raw]\n",
    "x_list = [xi.split() for xi in x]\n",
    "\n",
    "# stopword elemination\n",
    "stopw = readAFile('stopwords.txt')\n",
    "stopw = np.array([s.replace('\\n','') for s in stopw])\n",
    "x_list = [[xij for xij in xi #if((xij not in stopw) #and len(xij)>2)\\\n",
    "                             ] for xi in x_list]\n",
    "x_str = [' '.join(xi) for xi in x_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* avant pre-processing:\n",
      "\n",
      " Je ne sais ni pourquoi ni comment on s'est opposé il y a quelques douze années - douze ou treize ans - à la création de l'Université technologique.\n",
      "\n",
      "\n",
      "\n",
      "* après pre-processing:\n",
      "\n",
      "je ne sais ni pourquoi ni comment on s est oppose il y a quelques douze annees douze ou treize ans a la creation de l universite technologique\n"
     ]
    }
   ],
   "source": [
    "print('* avant pre-processing:\\n')\n",
    "print(x_raw[11]+'\\n\\n')\n",
    "\n",
    "print('* après pre-processing:\\n')\n",
    "print(x_str[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## du texte à la représentation vectorielle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countVe = CountVectorizer(max_df=0.78, min_df=1, ngram_range=(1,3))\n",
    "count = countVe.fit_transform(x_str)\n",
    "\n",
    "#Alternatives\n",
    "#count = tfidf_vectorizer.fit_transform(x_str)\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=1, #max_features=20000,\n",
    "\n",
    "X = count\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## application de modèles d'apprentissage\n",
    "\n",
    "### naive bayes multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf=sklearn.naive_bayes.MultinomialNB()\n",
    "nb_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm avec noyau linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = .9  # SVM regularization parameter\n",
    "svm_clf = svm.LinearSVC(C=C,max_iter=10000,tol=1e-6,class_weight={0:1.4,1:1.}).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modèle maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.9, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxent_clf = sklearn.linear_model.LogisticRegression(max_iter=100,C=1.9)\n",
    "maxent_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm avec noyau linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = .9  # SVM regularization parameter\n",
    "svm_clf = svm.LinearSVC(C=C,max_iter=10000,tol=1e-6,class_weight={0:1.4,1:1.}).fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## résultats des expériences\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Note**: Nous utilisons içi des données de validation pour confronter nos différents modèles et représentations vectorielles, afin d'éviter le phénomène \"surapprentissage\" qui pourrait apparaître lors de la modification des hyperparamètres des modèles pour se \"coller\" au données de test. L'utilisation de données de validation nous permet donc de vérifier si un modèle et/ou une représentation est efficace, et les données de test nous permettent d'y attester en mesurant le pouvoir de généralisation de ce modèle avec ces hyperparamètres. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Note bis**: Au lieu d'utiliser le taux de bonne prédiction pour évaluer nos modèles, nous utilisons le f1-score qui est une moyenne pondérée de la précision et du rappel:\n",
    "\n",
    "\n",
    ">* **rappel**: $\\Large\\frac{\\textrm{vrais positifs}}{\\textrm{vrai positifs + faux négatifs}}$\n",
    "\n",
    "\n",
    ">* **précision**: $\\Large\\frac{\\textrm{vrais positifs}}{\\textrm{vrais positifs + faux positifs}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "nb_scores = cross_validation.cross_val_score(nb_clf, X, y, scoring='f1_weighted')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (nb_scores.mean(), nb_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modèle maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "maxent_scores = cross_validation.cross_val_score(maxent_clf, X, y, scoring='f1_weighted')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (maxent_scores.mean(), maxent_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm avec noyau linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "svm_scores = cross_validation.cross_val_score(svm_clf, X, y, scoring='f1_weighted')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (svm_scores.mean(), svm_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chargement des données test et pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = readAFile('test.utf8')\n",
    "\n",
    "#extraction du texte\n",
    "test = []\n",
    "for txt in t:\n",
    "    txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "    test.append(txt)    \n",
    "\n",
    "#pre-processing\n",
    "test = [process(xi) for xi in test]\n",
    "test_list = [xi.split() for xi in test]\n",
    "stopw = readAFile('stopwords.txt')\n",
    "stopw = np.array([s.replace('\\n','') for s in stopw])\n",
    "test_list = [[xij for xij in xi #if(xij not in stopw #and len(xij)>2\\\n",
    "                                ] for xi in test_list]\n",
    "test_str = [' '.join(xi) for xi in test_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformation en représentation sac de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_test = countVe.transform(test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def post(y, rang, coeff):\n",
    "    '''for each label, we compute a count of the rang neighbors on each side, \n",
    "        and reassign the current label to the label that scored the highest\n",
    "        on the weighted count -- using coeff.\n",
    "        '''\n",
    "    y_new=np.zeros((len(y)))\n",
    "    for i in np.random.permutation(range(len(y))):\n",
    "        findex = i - rang\n",
    "        lindex = i + rang\n",
    "        if(i - rang<0):\n",
    "            findex = 0\n",
    "            lindex = rang + i\n",
    "        elif(i + rang > len(y)):\n",
    "            findex = i - rang\n",
    "            lindex = len(y) - 1\n",
    "        countM = len(np.where(y[findex:lindex] == 0)[0])\n",
    "        countC = len(np.where(y[findex:lindex] == 1)[0])\n",
    "        y_new[i] = np.argmax([countM * coeff, countC])\n",
    "    return y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test du post-processing sur les données de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dividing data in train and test, with ratio 0.4 for test data\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, \\\n",
    "                                        test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### naive bayes multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal accuracy: 0.936828021217\n",
      "accuracy after post-processing: 0.928499590691\n"
     ]
    }
   ],
   "source": [
    "nb_clf.fit(X_train, y_train)\n",
    "nb_preds = nb_clf.predict(X_test)\n",
    "nb_preds_post = post(post(nb_preds,5,2.45),5,1.)\n",
    "nb_score = f1_score(nb_preds, y_test, average='binary')\n",
    "nb_score_post = f1_score(nb_preds_post, y_test, average='binary')\n",
    "print(\"normal accuracy: \"+str(nb_score))\n",
    "print(\"accuracy after post-processing: \"+str(nb_score_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modèle maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal accuracy: 0.947266989473\n",
      "accuracy after post-processing: 0.927039834843\n"
     ]
    }
   ],
   "source": [
    "maxent_clf.fit(X_train, y_train)\n",
    "maxent_preds = maxent_clf.predict(X_test)\n",
    "maxent_preds_post = post(post(maxent_preds,5,2.45),5,1.)\n",
    "maxent_score = f1_score(maxent_preds, y_test, average='binary')\n",
    "maxent_score_post = f1_score(maxent_preds_post, y_test, average='binary')\n",
    "print(\"normal accuracy: \"+str(maxent_score))\n",
    "print(\"accuracy after post-processing: \"+str(maxent_score_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm avec noyau linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal accuracy: 0.944816865831\n",
      "accuracy after post-processing: 0.922938644212\n"
     ]
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)\n",
    "svm_preds = svm_clf.predict(X_test)\n",
    "svm_preds_post = post(post(svm_preds,5,2.45),5,1.)\n",
    "svm_score = f1_score(svm_preds, y_test, average='binary')\n",
    "svm_score_post = f1_score(svm_preds_post, y_test, average='binary')\n",
    "print(\"normal accuracy: \"+str(svm_score))\n",
    "print(\"accuracy after post-processing: \"+str(svm_score_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prédictions des labels des données de test\n",
    "### naive bayes multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_pred_labels = nb_clf.predict(count_test)\n",
    "nb_pred_labels = post(post(nb_pred_labels,5,2.45),5,1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modèle maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxent_pred_labels = maxent_clf.predict(count_test)\n",
    "maxent_pred_labels = post(post(maxent_pred_labels,5,2.45),5,1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm avec noyau linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_pred_labels = svm_clf.predict(count_test)\n",
    "svm_pred_labels = post(post(svm_pred_labels,5,2.45),5,1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print svm_pred_labels[150:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## écriture du fichier des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('test_labels_exemple.txt', 'w')\n",
    "for i in pred_labels:\n",
    "    if i == 1:\n",
    "        f.write('C\\n')\n",
    "    else:\n",
    "        f.write('M\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nous avons confrontés nos résultats sur les données de validations aux données de test, pour attester du pouvoir de généralisation des modèles et des représentations en donnant le document <code>test_labels_exemple.txt</code> au serveur, accessible depuis le lien suivant:\n",
    "http://webia.lip6.fr/~guigue/wikihomepage/pmwiki.php?n=Course.CourseAFDSoumission\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Le modèle ayant le meilleur score sur les données de test est le svm (avec les paramètres définis plus haut). Nous avons utilisé une représentation sac de mots (sous forme de comptage des occurences), en en utilisant à la fois les uni-grams et les bi-grams. Il peut être utile de noter que lorsque l'on effectuait un pre-processing où l'on prenait trop de distance avec les données d'origine (lemmatisation des mots du document, ou prélevement des stopwords, par exemple), l'on avait quasi-systématiquement une baisse du score. Nous avons donc effectués un pre-preoccessing minimal.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Nous avons eu un score de **0.8011** au f1-score. Le post processing nous augmente notre score de manière non négligeable: nous avons des des augmentations de résultats de 15 à 20% au f1-score sur les données de test, même si nous n'avons pas observé une hausse du f1-score sur les données de validations (il est possible que les données d'apprentissage ont étés mélangés, et leur distribution (au sens de la répartition) ne colle plus à la distribution dans les données de test.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
